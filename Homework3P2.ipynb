{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQh5DaHjwY9v"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EVoW7-9jwUmo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sAL7_-8A6TWB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 1.711473316227781\n",
      "Epoch 2/20, Training Loss: 1.485150503568518\n",
      "Epoch 3/20, Training Loss: 1.4233778502858376\n",
      "Epoch 4/20, Training Loss: 1.3878972135992893\n",
      "Epoch 5/20, Training Loss: 1.3628450302354782\n",
      "Epoch 6/20, Training Loss: 1.3431480968573006\n",
      "Epoch 7/20, Training Loss: 1.3279951620960262\n",
      "Epoch 8/20, Training Loss: 1.3146173407292134\n",
      "Epoch 9/20, Training Loss: 1.3034009359909486\n",
      "Epoch 10/20, Training Loss: 1.294143872883767\n",
      "Epoch 11/20, Training Loss: 1.2862671143062758\n",
      "Epoch 12/20, Training Loss: 1.2784827889909842\n",
      "Epoch 13/20, Training Loss: 1.2721875276175028\n",
      "Epoch 14/20, Training Loss: 1.266045561926979\n",
      "Epoch 15/20, Training Loss: 1.2608564193571565\n",
      "Epoch 16/20, Training Loss: 1.255879172915299\n",
      "Epoch 17/20, Training Loss: 1.251721989147566\n",
      "Epoch 18/20, Training Loss: 1.2477851554674453\n",
      "Epoch 19/20, Training Loss: 1.244573937858307\n",
      "Epoch 20/20, Training Loss: 1.2407389667464341\n",
      "Total execution time for training: 606.2889075279236 seconds\n",
      "Accuracy on test set: 58.054465986775746%\n"
     ]
    }
   ],
   "source": [
    "# LSTM for Sequences of 20\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "sequence_length = 20\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "encoded_text = [char_to_int[ch] for ch in text]\n",
    "sequences = []\n",
    "targets = []\n",
    "for i in range(0, len(encoded_text) - sequence_length):\n",
    "    seq = encoded_text[i:i+sequence_length]\n",
    "    target = encoded_text[i+sequence_length]\n",
    "    sequences.append(seq)\n",
    "    targets.append(target)\n",
    "\n",
    "sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "dataset = CharDataset(sequences, targets)\n",
    "\n",
    "batch_size = 128\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=batch_size)\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device),\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device))\n",
    "\n",
    "input_size = len(chars)\n",
    "hidden_size = 256\n",
    "output_size = len(chars)\n",
    "learning_rate = 0.001\n",
    "model = CharLSTM(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "epochs = 20\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total execution time for training: {execution_time} seconds\")\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UL_6IEvG7vC5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 1.697342387942904\n",
      "Epoch 2/20, Training Loss: 1.4726576827257034\n",
      "Epoch 3/20, Training Loss: 1.4113048192549644\n",
      "Epoch 4/20, Training Loss: 1.375155462476794\n",
      "Epoch 5/20, Training Loss: 1.3505117829527857\n",
      "Epoch 6/20, Training Loss: 1.3311707029569033\n",
      "Epoch 7/20, Training Loss: 1.3150749606162104\n",
      "Epoch 8/20, Training Loss: 1.3021008289807567\n",
      "Epoch 9/20, Training Loss: 1.291757515019818\n",
      "Epoch 10/20, Training Loss: 1.2821564125778753\n",
      "Epoch 11/20, Training Loss: 1.2734858036400325\n",
      "Epoch 12/20, Training Loss: 1.2663496789820086\n",
      "Epoch 13/20, Training Loss: 1.2603301568511431\n",
      "Epoch 14/20, Training Loss: 1.254051627652165\n",
      "Epoch 15/20, Training Loss: 1.2488141292668729\n",
      "Epoch 16/20, Training Loss: 1.2438960185576908\n",
      "Epoch 17/20, Training Loss: 1.2387259014450949\n",
      "Epoch 18/20, Training Loss: 1.2359358922835404\n",
      "Epoch 19/20, Training Loss: 1.2318230640406043\n",
      "Epoch 20/20, Training Loss: 1.2287620891802487\n",
      "Total execution time for training: 748.9822533130646 seconds\n",
      "Accuracy on test set: 58.84441416038696%\n"
     ]
    }
   ],
   "source": [
    "# LSTM for Sequences of 30\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "sequence_length = 30\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encoded_text = [char_to_int[ch] for ch in text]\n",
    "\n",
    "sequences = []\n",
    "targets = []\n",
    "for i in range(0, len(encoded_text) - sequence_length):\n",
    "    seq = encoded_text[i:i+sequence_length]\n",
    "    target = encoded_text[i+sequence_length]\n",
    "    sequences.append(seq)\n",
    "    targets.append(target)\n",
    "\n",
    "sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "dataset = CharDataset(sequences, targets)\n",
    "\n",
    "batch_size = 128\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device),\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device))\n",
    "\n",
    "input_size = len(chars)\n",
    "hidden_size = 256\n",
    "output_size = len(chars)\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = CharLSTM(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 20\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total execution time for training: {execution_time} seconds\")\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpN5GhcJ8NIa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 1.7001969526324001\n",
      "Epoch 2/20, Training Loss: 1.4711108197271183\n",
      "Epoch 3/20, Training Loss: 1.4074978094295216\n",
      "Epoch 4/20, Training Loss: 1.3704794130662543\n",
      "Epoch 5/20, Training Loss: 1.3441443178835326\n",
      "Epoch 6/20, Training Loss: 1.3238139850151724\n",
      "Epoch 7/20, Training Loss: 1.3077308766606923\n",
      "Epoch 8/20, Training Loss: 1.294043606399139\n",
      "Epoch 9/20, Training Loss: 1.2827759207945235\n",
      "Epoch 10/20, Training Loss: 1.2727265480669032\n",
      "Epoch 11/20, Training Loss: 1.2645177987739904\n",
      "Epoch 12/20, Training Loss: 1.2569114672215935\n",
      "Epoch 13/20, Training Loss: 1.2498197430450046\n",
      "Epoch 14/20, Training Loss: 1.2443295110932486\n",
      "Epoch 15/20, Training Loss: 1.2378445891255954\n",
      "Epoch 16/20, Training Loss: 1.2333719689848432\n",
      "Epoch 17/20, Training Loss: 1.228789914083556\n",
      "Epoch 18/20, Training Loss: 1.2248058697360265\n",
      "Epoch 19/20, Training Loss: 1.221132945431269\n",
      "Epoch 20/20, Training Loss: 1.2182156594082438\n",
      "Total execution time for training: 870.7704834938049 seconds\n",
      "Accuracy on test set: 59.194688638941315%\n"
     ]
    }
   ],
   "source": [
    "# LSTM for Sequences of 50\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "sequence_length = 50\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "encoded_text = [char_to_int[ch] for ch in text]\n",
    "sequences = []\n",
    "targets = []\n",
    "for i in range(0, len(encoded_text) - sequence_length):\n",
    "    seq = encoded_text[i:i+sequence_length]\n",
    "    target = encoded_text[i+sequence_length]\n",
    "    sequences.append(seq)\n",
    "    targets.append(target)\n",
    "\n",
    "sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "dataset = CharDataset(sequences, targets)\n",
    "\n",
    "batch_size = 128\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device),\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device))\n",
    "input_size = len(chars)\n",
    "hidden_size = 256\n",
    "output_size = len(chars)\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = CharLSTM(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 20\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total execution time for training: {execution_time} seconds\")\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5H172S17-4z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 1.6957282759954515\n",
      "Epoch 2/20, Training Loss: 1.4996592183942024\n",
      "Epoch 3/20, Training Loss: 1.4534013345880339\n",
      "Epoch 4/20, Training Loss: 1.4285255410995419\n",
      "Epoch 5/20, Training Loss: 1.4115607911016637\n",
      "Epoch 6/20, Training Loss: 1.399496109439119\n",
      "Epoch 7/20, Training Loss: 1.3905963871886657\n",
      "Epoch 8/20, Training Loss: 1.38464664220297\n",
      "Epoch 9/20, Training Loss: 1.379856461775556\n",
      "Epoch 10/20, Training Loss: 1.3759207847179247\n",
      "Epoch 11/20, Training Loss: 1.374044992769168\n",
      "Epoch 12/20, Training Loss: 1.37172578532533\n",
      "Epoch 13/20, Training Loss: 1.369672107888034\n",
      "Epoch 14/20, Training Loss: 1.3691545740086255\n",
      "Epoch 15/20, Training Loss: 1.3696721814192374\n",
      "Epoch 16/20, Training Loss: 1.3690125433641538\n",
      "Epoch 17/20, Training Loss: 1.3704669377411225\n",
      "Epoch 18/20, Training Loss: 1.3722592879832647\n",
      "Epoch 19/20, Training Loss: 1.373418925542703\n",
      "Epoch 20/20, Training Loss: 1.3763837425601393\n",
      "Total execution time for training: 474.571594953537 seconds\n",
      "Accuracy on test set: 56.455900481900706%\n"
     ]
    }
   ],
   "source": [
    "# GRU for Sequences of 20\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "sequence_length = 20\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encoded_text = [char_to_int[ch] for ch in text]\n",
    "\n",
    "sequences = []\n",
    "targets = []\n",
    "for i in range(0, len(encoded_text) - sequence_length):\n",
    "    seq = encoded_text[i:i+sequence_length]\n",
    "    target = encoded_text[i+sequence_length]\n",
    "    sequences.append(seq)\n",
    "    targets.append(target)\n",
    "\n",
    "sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "dataset = CharDataset(sequences, targets)\n",
    "\n",
    "batch_size = 128\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "class CharGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "input_size = len(chars)\n",
    "hidden_size = 256\n",
    "output_size = len(chars)\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = CharGRU(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Step 4: Training the model\n",
    "epochs = 20\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total execution time for training: {execution_time} seconds\")\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUSNsLXm8G0o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 1.6861268352723573\n",
      "Epoch 2/20, Training Loss: 1.4944427352541831\n",
      "Epoch 3/20, Training Loss: 1.4488336394828965\n",
      "Epoch 4/20, Training Loss: 1.4227918446388863\n",
      "Epoch 5/20, Training Loss: 1.407423982812424\n",
      "Epoch 6/20, Training Loss: 1.396357479455549\n",
      "Epoch 7/20, Training Loss: 1.3875296961898715\n",
      "Epoch 8/20, Training Loss: 1.3806749197108883\n",
      "Epoch 9/20, Training Loss: 1.3746316732087493\n",
      "Epoch 10/20, Training Loss: 1.3712931449855803\n",
      "Epoch 11/20, Training Loss: 1.3683031995834734\n",
      "Epoch 12/20, Training Loss: 1.3650096261973226\n",
      "Epoch 13/20, Training Loss: 1.3632348598297888\n",
      "Epoch 14/20, Training Loss: 1.3652427787419645\n",
      "Epoch 15/20, Training Loss: 1.366415409532622\n",
      "Epoch 16/20, Training Loss: 1.3651240237283762\n",
      "Epoch 17/20, Training Loss: 1.3639760853501481\n",
      "Epoch 18/20, Training Loss: 1.3656101293489977\n",
      "Epoch 19/20, Training Loss: 1.3670186329541094\n",
      "Epoch 20/20, Training Loss: 1.3685405236325316\n",
      "Total execution time for training: 390.2198209762573 seconds\n",
      "Accuracy on test set: 56.78051579527778%\n"
     ]
    }
   ],
   "source": [
    "# GRU for Sequences of 30\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "sequence_length = 30\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encoded_text = [char_to_int[ch] for ch in text]\n",
    "\n",
    "sequences = []\n",
    "targets = []\n",
    "for i in range(0, len(encoded_text) - sequence_length):\n",
    "    seq = encoded_text[i:i+sequence_length]\n",
    "    target = encoded_text[i+sequence_length]\n",
    "    sequences.append(seq)\n",
    "    targets.append(target)\n",
    "\n",
    "sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "dataset = CharDataset(sequences, targets)\n",
    "\n",
    "batch_size = 128\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "# Step 2: Define the GRU model\n",
    "class CharGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "input_size = len(chars)\n",
    "hidden_size = 256\n",
    "output_size = len(chars)\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = CharGRU(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "epochs = 20\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total execution time for training: {execution_time} seconds\")\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKKMyHpi8Tpf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 1.683142794476505\n",
      "Epoch 2/20, Training Loss: 1.4841036172221644\n",
      "Epoch 3/20, Training Loss: 1.4351261619720546\n",
      "Epoch 4/20, Training Loss: 1.4081026340485165\n",
      "Epoch 5/20, Training Loss: 1.3900087800605314\n",
      "Epoch 6/20, Training Loss: 1.3779923070060216\n",
      "Epoch 7/20, Training Loss: 1.3691461610753026\n",
      "Epoch 8/20, Training Loss: 1.3626731731357746\n",
      "Epoch 9/20, Training Loss: 1.3562186070672582\n",
      "Epoch 10/20, Training Loss: 1.3521321563138593\n",
      "Epoch 11/20, Training Loss: 1.3495454395831425\n",
      "Epoch 12/20, Training Loss: 1.3487541041728535\n",
      "Epoch 13/20, Training Loss: 1.3455588955342932\n",
      "Epoch 14/20, Training Loss: 1.3471375236875818\n",
      "Epoch 15/20, Training Loss: 1.3462522669102537\n",
      "Epoch 16/20, Training Loss: 1.3446821476000315\n",
      "Epoch 17/20, Training Loss: 1.3478725592403633\n",
      "Epoch 18/20, Training Loss: 1.3475939247550384\n",
      "Epoch 19/20, Training Loss: 1.350850970634616\n",
      "Epoch 20/20, Training Loss: 1.353002951667087\n",
      "Total execution time for training: 493.34515857696533 seconds\n",
      "Accuracy on test set: 56.89315861908199%\n"
     ]
    }
   ],
   "source": [
    "# GRU for Sequences of 50\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "sequence_length = 50\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encoded_text = [char_to_int[ch] for ch in text]\n",
    "\n",
    "sequences = []\n",
    "targets = []\n",
    "for i in range(0, len(encoded_text) - sequence_length):\n",
    "    seq = encoded_text[i:i+sequence_length]\n",
    "    target = encoded_text[i+sequence_length]\n",
    "    sequences.append(seq)\n",
    "    targets.append(target)\n",
    "\n",
    "sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "dataset = CharDataset(sequences, targets)\n",
    "\n",
    "batch_size = 128\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "class CharGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device))\n",
    "input_size = len(chars)\n",
    "hidden_size = 256\n",
    "output_size = len(chars)\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = CharGRU(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "epochs = 20\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total execution time for training: {execution_time} seconds\")\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        hidden = model.init_hidden(data.size(0))\n",
    "        output, hidden = model(data, hidden)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * correct / total}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
